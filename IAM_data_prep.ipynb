{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1glPohmWrUwvdg2Gk6Na-JVH-Q-FVeVW_","authorship_tag":"ABX9TyOpKscr10uCaJRvjnoEIFnq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Dataset Preperation"],"metadata":{"id":"SWC5rbxSVOie"}},{"cell_type":"markdown","source":["### Set Up"],"metadata":{"id":"q0oSzWnjh-yF"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_bJpp9IcVfpT","executionInfo":{"status":"ok","timestamp":1704251336883,"user_tz":-660,"elapsed":4354,"user":{"displayName":"Yuchen Li","userId":"14923965350100155980"}},"outputId":"cc153e78-ef2c-4966-9575-4965b22682e7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import os\n","import glob\n","import xml.etree.ElementTree as ET\n","import shutil\n","from sklearn.model_selection import train_test_split\n","from PIL import Image\n","import re"],"metadata":{"id":"_T_YCCw1VWwg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Helpers"],"metadata":{"id":"N7v4pjNOXI8x"}},{"cell_type":"code","source":["def copy_files(file_paths, destination_directory):\n","    \"\"\"Copy files to the specified destination directory.\"\"\"\n","    for file_path in file_paths:\n","        shutil.copy(file_path, destination_directory)\n","\n","def convert_xml_to_txt(xml_file, output_directory):\n","    tree = ET.parse(xml_file)\n","    root = tree.getroot()\n","\n","    # Extract base name and create corresponding .txt file path\n","    base_name = os.path.basename(xml_file).replace('.xml', '.txt')\n","    txt_file_path = os.path.join(output_directory, base_name)\n","\n","    with open(txt_file_path, 'w') as file:\n","        for line in root.findall('.//handwritten-part/line'):\n","            line_text = line.get('text')\n","            if not line_text:\n","                continue\n","\n","            # Initialize bounding box coordinates\n","            x_min, y_min, x_max, y_max = float('inf'), float('inf'), 0, 0\n","\n","            for word in line.findall('word'):\n","                for cmp in word.findall('cmp'):\n","                    x, y, width, height = int(cmp.get('x')), int(cmp.get('y')), int(cmp.get('width')), int(cmp.get('height'))\n","                    x_min = min(x_min, x)\n","                    y_min = min(y_min, y)\n","                    x_max = max(x_max, x + width)\n","                    y_max = max(y_max, y + height)\n","\n","            if x_min < float('inf'):\n","                file.write(f\"{x_min},{y_min},{x_max},{y_min},{x_max},{y_max},{x_min},{y_max},{line_text}\\n\")"],"metadata":{"id":"aCyixU-BXIXS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Convert label structure to OCR readible"],"metadata":{"id":"5j3_I6zyWU1-"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"O4Kb1gYeVNQb"},"outputs":[],"source":["label_directory = \"/content/drive/My Drive/ocr_project/datasets/IAM_data/xml\"\n","output_directory = \"/content/drive/My Drive/ocr_project/datasets/IAM_data/txt_labels\"\n","\n","os.makedirs(output_directory, exist_ok=True)\n","\n","# Process each XML file and create corresponding TXT file\n","for xml_path in glob.glob(os.path.join(label_directory, \"*.xml\")):\n","    convert_xml_to_txt(xml_path, output_directory)"]},{"cell_type":"markdown","source":["#### Image Preprocessing"],"metadata":{"id":"AkFKrRwgudUl"}},{"cell_type":"code","source":["# Replace 'path_to_your_image.png' with the path to an actual image file\n","image = Image.open('/content/drive/My Drive/ocr_project/datasets/IAM_data/forms/h07-087.png')\n","print(image.size)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t4ygc-ituduH","executionInfo":{"status":"ok","timestamp":1704090865593,"user_tz":-660,"elapsed":8606,"user":{"displayName":"Yuchen Li","userId":"14923965350100155980"}},"outputId":"8c1d8865-17e9-49f1-fb3c-82a59b05cb0c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(2479, 3542)\n"]}]},{"cell_type":"code","source":["# Open the image\n","image = Image.open('/content/drive/My Drive/ocr_project/datasets/IAM_data/forms/h07-087.png')\n","\n","# Calculate the new size, maintaining the aspect ratio\n","target_width = 620\n","target_height = 886\n","\n","# Resize the image, maintaining aspect ratio\n","aspect_ratio = image.width / image.height\n","new_width = int(target_height * aspect_ratio)\n","image = image.resize((new_width, target_height), Image.ANTIALIAS)\n","\n","# Pad the resized image to the required input size\n","new_image = Image.new('RGB', (target_width, target_height), (255, 255, 255))  # Assuming white padding\n","offset = ((target_width - new_width) // 2, 0)\n","new_image.paste(image, offset)\n","\n","# Save or process the padded image\n","new_image.save('/content/drive/My Drive/ocr_project/test_reshape/resized_and_padded_image.png')\n","image = Image.open('/content/drive/My Drive/ocr_project/test_reshape/resized_and_padded_image.png')\n","print(image.size)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BhmH4zpfuyFg","executionInfo":{"status":"ok","timestamp":1704253140576,"user_tz":-660,"elapsed":329,"user":{"displayName":"Yuchen Li","userId":"14923965350100155980"}},"outputId":"651c14cb-0702-4769-b376-cfd94982ec73"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-9-f94b58baeeae>:11: DeprecationWarning: ANTIALIAS is deprecated and will be removed in Pillow 10 (2023-07-01). Use LANCZOS or Resampling.LANCZOS instead.\n","  image = image.resize((new_width, target_height), Image.ANTIALIAS)\n"]},{"output_type":"stream","name":"stdout","text":["(620, 886)\n"]}]},{"cell_type":"markdown","source":["#### Train/Test Split"],"metadata":{"id":"ATyFo59FWn-V"}},{"cell_type":"code","source":["\n","# Directory containing the image files\n","image_directory = \"/content/drive/My Drive/ocr_project/datasets/IAM_data/forms\"\n","# Directory to save the training, validation and test images\n","train_image_directory = \"/content/drive/My Drive/ocr_project/datasets/IAM_data/train/train_image\"\n","val_image_directory = \"/content/drive/My Drive/ocr_project/datasets/IAM_data/validation/val_image\"\n","test_image_directory = \"/content/drive/My Drive/ocr_project/datasets/IAM_data/test/test_image\"\n","\n","# Make the directories if they don't exist\n","os.makedirs(train_image_directory, exist_ok=True)\n","os.makedirs(val_image_directory, exist_ok=True)\n","os.makedirs(test_image_directory, exist_ok=True)\n","\n","# Directory containing the annotation files\n","label_directory = \"/content/drive/My Drive/ocr_project/datasets/IAM_data/txt_labels\"\n","# Directory to save the training, validation and test labels\n","train_label_directory = \"/content/drive/My Drive/ocr_project/datasets/IAM_data/train/train_label\"\n","val_label_directory = \"/content/drive/My Drive/ocr_project/datasets/IAM_data/validation/val_label\"\n","test_label_directory = \"/content/drive/My Drive/ocr_project/datasets/IAM_data/test/test_label\"\n","\n","# Make the directories if they don't exist\n","os.makedirs(train_label_directory, exist_ok=True)\n","os.makedirs(val_label_directory, exist_ok=True)\n","os.makedirs(test_label_directory, exist_ok=True)\n","\n","# Get list of all image files\n","image_files = glob.glob(os.path.join(image_directory, \"*.png\"))\n","# Get list of all annotation files\n","label_files = [os.path.join(label_directory, os.path.basename(f).replace('.png', '.txt')) for f in image_files]\n","\n","# Split into training and temp (validation + test)\n","train_images, temp_images, train_labels, temp_labels = train_test_split(\n","    image_files, label_files, test_size=0.3, random_state=42)\n","\n","# Split the temp into validation and test\n","val_images, test_images, val_labels, test_labels = train_test_split(\n","    temp_images, temp_labels, test_size=1/3, random_state=42)  # 1/3 of 30% will give 10% test size overall\n","\n","# Copy the files to their new directories\n","copy_files(train_images, train_image_directory)\n","copy_files(val_images, val_image_directory)\n","copy_files(test_images, test_image_directory)\n","copy_files(train_labels, train_label_directory)\n","copy_files(val_labels, val_label_directory)\n","copy_files(test_labels, test_label_directory)\n","\n","print(f\"Training images: {len(train_images)}\")\n","print(f\"Validation images: {len(val_images)}\")\n","print(f\"Test images: {len(test_images)}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TEAQhDDNWrjG","executionInfo":{"status":"ok","timestamp":1704085764079,"user_tz":-660,"elapsed":159260,"user":{"displayName":"Yuchen Li","userId":"14923965350100155980"}},"outputId":"ed92999e-feeb-45de-d7ea-3bea42751589"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training images: 735\n","Validation images: 210\n","Test images: 106\n"]}]},{"cell_type":"markdown","source":["#### Test Hand Writing / Machine-Typed Isolation"],"metadata":{"id":"iyEAniWc20t7"}},{"cell_type":"code","source":["\n","# Set up the directories\n","output_dir = \"/content/drive/My Drive/ocr_project/datasets/IAM_data/test/test_output\"\n","xml_dir = \"/content/drive/My Drive/ocr_project/datasets/IAM_data/xml\"\n","printed_output_dir = \"/content/drive/My Drive/ocr_project/datasets/IAM_data/test/printed_output\"\n","writing_output_dir = \"/content/drive/My Drive/ocr_project/datasets/IAM_data/test/writing_output\"\n","printed_label_dir = \"/content/drive/My Drive/ocr_project/datasets/IAM_data/test/printed_label\"\n","\n","# Make sure the new directories exist\n","os.makedirs(printed_output_dir, exist_ok=True)\n","os.makedirs(writing_output_dir, exist_ok=True)\n","os.makedirs(printed_label_dir, exist_ok=True)\n","\n","# List all the files in the output directory\n","for output_file in os.listdir(output_dir):\n","    if output_file.endswith('_result.txt'):\n","        base_name = output_file.replace('_result.txt', '')\n","        xml_file_path = os.path.join(xml_dir, base_name + '.xml')\n","        output_file_path = os.path.join(output_dir, output_file)\n","        printed_output_file_path = os.path.join(printed_output_dir, base_name + '_printed_output.txt')\n","        writing_output_file_path = os.path.join(writing_output_dir, base_name + '_writing_output.txt')\n","        printed_label_file_path = os.path.join(printed_label_dir, base_name + '_printed_label.txt')\n","\n","        # Parse the XML file to determine how many lines to extract\n","        tree = ET.parse(xml_file_path)\n","        root = tree.getroot()\n","        machine_printed_part = root.find('machine-printed-part')\n","        if machine_printed_part is not None:\n","            machine_print_lines = machine_printed_part.findall('machine-print-line')\n","            x = len(machine_print_lines)  # This is how many lines we want to extract\n","\n","            # Extract printed section from the OCR output file\n","            with open(output_file_path, 'r') as file:\n","                lines = file.readlines()[2:x+2]  # Skip the first 2 line and take x lines\n","                stripped_lines = [line.strip() for line in lines]\n","                single_line_output_p = ' '.join(stripped_lines)\n","\n","            # Write the single line output to the new output file\n","            with open(printed_output_file_path, 'w') as file:\n","              file.write(single_line_output_p + '\\n')\n","\n","            # Extract writing from the OCR output file\n","            with open(output_file_path, 'r') as file:\n","                lines = file.readlines()[x+2:-1]\n","                stripped_lines = [line.strip() for line in lines]\n","                single_line_output_w = ' '.join(stripped_lines)\n","            # Write the single line output to the new output file\n","            with open(writing_output_file_path, 'w') as file:\n","              file.write(single_line_output_w + '\\n')\n","\n","            # Process the XML file to extract only the machine printed text\n","            printed_text = ' '.join([line.attrib['text'] for line in machine_print_lines])\n","\n","            # Write the processed text to the new label file\n","            with open(printed_label_file_path, 'w') as file:\n","                file.write(printed_text)\n","\n","print(\"Preprocessing completed.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q5vwwLNW209u","executionInfo":{"status":"ok","timestamp":1704251386717,"user_tz":-660,"elapsed":2979,"user":{"displayName":"Yuchen Li","userId":"14923965350100155980"}},"outputId":"d6f16712-35ea-411b-cc63-d2fcfc69a82f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Preprocessing completed.\n"]}]}]}